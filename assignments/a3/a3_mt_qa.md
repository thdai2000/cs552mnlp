
# Part 3 - Open-Answer Questions

#### Q1: How is BERTScore calculated? Read the first three paragraphs in Section 3 -- called "Token representation", "Similarity Measure", and "BERTScore" -- in [this paper](https://arxiv.org/pdf/1904.09675.pdf) and give a technical description of how the BERTScore precision/recall/f1 is calculated in ~6 sentences. You do not need to describe anything outside the scope of these specific paragraphs.

We use contextual embeddings from BERT (or its variations) to represent the tokens in a sentence. These embeddings are normalized in the first place. Then, we use cosine similarity, i.e. taking the inner product, to measure the similarity of any pair of embeddings. To calculate BERTScore precision, we use greedy matching, where we find the highest similarity score from the reference sentence for each token in the candidate sentence, and take the average. To calculate BERTScore recall, we also use greedy matching, where we find the highest similarity score from the candidate sentence for each token in the reference sentence, and take the average. The BERTScore f1 is calucalted by taking the harmonic mean of BERTScore precision and recall. 
#### Q2: How is COMET trained and calculated? Read Section 2.4 -- "Translation Ranking Model" -- in [this paper](https://arxiv.org/pdf/2009.09025.pdf) and give a technical description in ~6 sentences.

In general, COMET is aimed to enhance the similarity of the source or reference sentence to the better hypothesis, and reduce the similarity to the worse hypothesis. More specifically, during training phase, four sequences (source, reference, better hypothesis, worse hypothesis) are passed through a BERT-based pre-trained cross-lingual encoder and a pooling layer, to be represented as four embeddings. The euclidean distances between the embedding of the source and the embeddings of two hypotheses are calculated, and a margin loss is designed so that the distance to the worse hypothesis is at least \epsilon larger than to the better hypothesis. The same margin loss applies to the reference and adds up to the previous loss to train the model. During inference, only three sequences (one hypothesis, the source and the reference) are passed to the model, and the harmonic mean of the distance to the source and the reference is calculated. This harmonic mean is further converted to a similarity score ranging from 0 to 1 to be used as a metric of the quality of the hypothesis.



#### Q3: Given your understanding of BLEU, BERTScore and COMET, how would you interpret the Kendall's Tau correlation results? Which ones are the least and most correlated? What is your hypothesis regarding the reasons behind the lowest correlation for one metric and the highest correlation in the other?

The higher Kendall's Tau is, the more correlated the metric is to the human evaluation. Therefore, BLEU (BLEU, BLEU-1, BLEU-4) is the least correlated, and COMET is the most correlated. The potential reason is that BLEU only does the exact-string matching and does not say much about how human perceive the quality of the sentence. BERTScore alleviates this problem by evaluating in a "deeper" semantic structure, i.e. contextual embeddings, and each embedding contains some information about the whole sentence, thus allowing more flexibility in comparing the similarities between sentences. COMET is even better than BERTScore because it utilizes both source and reference sentences to evaluate the hypothesis, making the evaluation more robust.


#### Q4: Assume you have a large set of story beginnings and you would like to evaluate how well a model completes the stories. What problem would you run into with BLEU and COMET? Would the same disadvantages apply to BERTScore and why? Give your justification. Answer in ~6 sentences.

BLEU is a poor metric for story generation. Story can be rather open-ended, and completions that rarely overlap in tokens can be both perceived as high-quality, and even though they have many overlappings, they are not necessarily good due to the intrinsic limitation (only evaluating on string level) of BLEU. COMET could be even worse because its training aim does not apply to story generation at all. The story beginning can not be used as source to calculate the similarity, which is different from the MT task where source, reference, and hypothesis are expected to share similar semantic meanings as much as possible. BERTScore is the best among these three metrics because it's a "soft" measure of the similarity between the completion and the reference. However, for open-ended tasks like story generation, automatic metrics always fall short of telling the real quality of generation, and human judgement is crucial.
